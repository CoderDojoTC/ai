{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Teaching AI Concepts to Students 8-18 A repository for mentors who would like to teach AI to kids 8-18. It contains lesson plans and resources to make understanding AI, Machine Learning and Prediction fun and easy to understand for a wide variety of ages and backgrounds. Key Concepts Here are some key concepts we will be learning in this course. Theses questions and answers are just the beginning of our understanding of the areas around AI. What is AI? In this courese, We will define AI is the ability to learn rules from data and make predictins about the future. We will contrast AI systems from systems where we write the rules on our own. We will learn how to teach computers how to classify images and sounds. What is Prediction? Prediction is using data from the past to figure out the probability of something in the future. Prediciton is the central thing we do in AI and specificlly machine learning. What is Recomentation? Recomendation is a prediction of if a person will like a specific action. When we search for a page in a search engine like Google, the search engine is predicting what web pages it thinks we would like to see. What is Machine Learning? How is Machine Learning Different from Codeing? What is Training? What is Bias? What is Explainability? When do recomendations need to be explainable? What is a Neural Network? What is Deep Learning? Resourcese Resources","title":"CoderDojo TC AI"},{"location":"#teaching-ai-concepts-to-students-8-18","text":"A repository for mentors who would like to teach AI to kids 8-18. It contains lesson plans and resources to make understanding AI, Machine Learning and Prediction fun and easy to understand for a wide variety of ages and backgrounds.","title":"Teaching AI Concepts to Students 8-18"},{"location":"#key-concepts","text":"Here are some key concepts we will be learning in this course. Theses questions and answers are just the beginning of our understanding of the areas around AI.","title":"Key Concepts"},{"location":"#what-is-ai","text":"In this courese, We will define AI is the ability to learn rules from data and make predictins about the future. We will contrast AI systems from systems where we write the rules on our own. We will learn how to teach computers how to classify images and sounds.","title":"What is AI?"},{"location":"#what-is-prediction","text":"Prediction is using data from the past to figure out the probability of something in the future. Prediciton is the central thing we do in AI and specificlly machine learning.","title":"What is Prediction?"},{"location":"#what-is-recomentation","text":"Recomendation is a prediction of if a person will like a specific action. When we search for a page in a search engine like Google, the search engine is predicting what web pages it thinks we would like to see.","title":"What is Recomentation?"},{"location":"#what-is-machine-learning","text":"","title":"What is Machine Learning?"},{"location":"#how-is-machine-learning-different-from-codeing","text":"","title":"How is Machine Learning Different from Codeing?"},{"location":"#what-is-training","text":"","title":"What is Training?"},{"location":"#what-is-bias","text":"","title":"What is Bias?"},{"location":"#what-is-explainability","text":"","title":"What is Explainability?"},{"location":"#when-do-recomendations-need-to-be-explainable","text":"","title":"When do recomendations need to be explainable?"},{"location":"#what-is-a-neural-network","text":"","title":"What is a Neural Network?"},{"location":"#what-is-deep-learning","text":"","title":"What is Deep Learning?"},{"location":"#resourcese","text":"Resources","title":"Resourcese"},{"location":"contact/","text":"CoderDojo TC AI Mentor Contacts General Code Savvy Contact kidscode@codesavvy.org Specific questions on this repository Dan McCreary on LinkedIn","title":"Contact"},{"location":"contact/#coderdojo-tc-ai-mentor-contacts","text":"","title":"CoderDojo TC AI Mentor Contacts"},{"location":"contact/#general-code-savvy-contact","text":"kidscode@codesavvy.org","title":"General Code Savvy Contact"},{"location":"contact/#specific-questions-on-this-repository","text":"Dan McCreary on LinkedIn","title":"Specific questions on this repository"},{"location":"deep-learning-glossary/","text":"Deep Learning Glossary Core Concepts Code vs. neural networks Detrministic Functions vs. Probalistic Functions Neural Network Training Weights Error Functions Gradient Decent Testing a Neural Network Types of Neural Networks Detailed Concepts Image recognition Digit recognition Image pixels Pixel brightness Grayscale values Neural networks - overall components Input layers Hidden layers Output layers Weights Bias Nodes Counting Parameters activation activation functions probability decomposition training weighted sums normalization the sigmoid function bias parameter counting (sum of weights and bias) vector representations matrix representations linear algebra transformation functions. review of prior video in handwritten digit recognition and the structure of a neural network model the training process labels in the training set generalization test data (what you network has never seen before) testing the precision of test data MNIST training database (LeCun, Cortes, and Burges) MNIST database Finding minima like in Calculus where the slop is zero Slop of zero is a miniumum of an error Random initialization of weights A cost function - a way of telling a neural network how it can improve Adding the squares of the differences Averaging the cost of the error functions for all input data Finding the lowest cost and minimumization using a 2D visualization Using the slope as a hint on how to adjust the weights The setting slope (derative) to zero to find a minium cost Balls rolling down a hill metaphore and local miniums The role of step size when moving toward a minimum cost Moving to a 2D input model and a 3D error function Gradient function and direction of steepest decent Finding the right downhill direction vector Reference to the backpropagation algorithm (next video) Association of learning to cost function minimumization Why smoothness of a cost function is critical for gradient decent to work and how using decimal values for activation node values is important How our hope of looking for components of a line in each layer didn't work at all The neural network does not \"understand\" the shape - it can't draw any shape. There is no interpretpetable knowledge in the individual weights. This is old technology - based on a multi-layer perceptron - we can do better with modern algorithms This primitive approach is \"dumb\" - but required for us to understand modern variations like CNN and LSTM","title":"Deep Learning Glossary"},{"location":"deep-learning-glossary/#deep-learning-glossary","text":"","title":"Deep Learning Glossary"},{"location":"deep-learning-glossary/#core-concepts","text":"Code vs. neural networks Detrministic Functions vs. Probalistic Functions Neural Network Training Weights Error Functions Gradient Decent Testing a Neural Network Types of Neural Networks","title":"Core Concepts"},{"location":"deep-learning-glossary/#detailed-concepts","text":"Image recognition Digit recognition Image pixels Pixel brightness Grayscale values Neural networks - overall components Input layers Hidden layers Output layers Weights Bias Nodes Counting Parameters activation activation functions probability decomposition training weighted sums normalization the sigmoid function bias parameter counting (sum of weights and bias) vector representations matrix representations linear algebra transformation functions. review of prior video in handwritten digit recognition and the structure of a neural network model the training process labels in the training set generalization test data (what you network has never seen before) testing the precision of test data MNIST training database (LeCun, Cortes, and Burges) MNIST database Finding minima like in Calculus where the slop is zero Slop of zero is a miniumum of an error Random initialization of weights A cost function - a way of telling a neural network how it can improve Adding the squares of the differences Averaging the cost of the error functions for all input data Finding the lowest cost and minimumization using a 2D visualization Using the slope as a hint on how to adjust the weights The setting slope (derative) to zero to find a minium cost Balls rolling down a hill metaphore and local miniums The role of step size when moving toward a minimum cost Moving to a 2D input model and a 3D error function Gradient function and direction of steepest decent Finding the right downhill direction vector Reference to the backpropagation algorithm (next video) Association of learning to cost function minimumization Why smoothness of a cost function is critical for gradient decent to work and how using decimal values for activation node values is important How our hope of looking for components of a line in each layer didn't work at all The neural network does not \"understand\" the shape - it can't draw any shape. There is no interpretpetable knowledge in the individual weights. This is old technology - based on a multi-layer perceptron - we can do better with modern algorithms This primitive approach is \"dumb\" - but required for us to understand modern variations like CNN and LSTM","title":"Detailed Concepts"},{"location":"glossary/","text":"Glossary of AI Terms Activation Function A mathematical function applied to a node in a neural network, dictating the output given an input or set of inputs. Algorithm A set of step-by-step procedures or formulas for solving a problem or performing a task. Artificial Intelligence (AI) The simulation of human intelligence processes by machines, especially computer systems. Artificial Neural Network (ANN) A computing model inspired by biological neural networks, designed to recognize patterns. Backpropagation An algorithm for adjusting the weights in a neural network based on the error in the output. Batch Size The number of training examples utilized in one iteration during the training of a neural network. Bias In machine learning, a term that describes the skewness in data or algorithmic decisions. In neural networks, a constant added to a weighted sum before the activation function. Classification A type of supervised learning where an algorithm categorizes input data into predefined classes. Clustering A type of unsupervised learning that groups similar data points together based on their features. Convolutional Neural Network (CNN) A type of neural network commonly used in image processing that uses convolutional layers to filter inputs for useful information. Decision Tree A flowchart-like tree structure used in machine learning for making decisions based on multiple conditions. Data Mining The process of discovering patterns and knowledge from large amounts of data. Data Set A collection of related data points used for the purposes of analysis or training machine learning models. Deep Learning A subset of machine learning involving algorithms inspired by the structure and function of the brain called artificial neural networks. Epoch One complete forward and backward pass of all the training examples in a data set. Feature An individual measurable property of the data being observed, often used as input variables for machine learning models. Heuristic A problem-solving technique that employs a practical method to find a sufficient, though not always optimal, solution. Machine Learning (ML) A subset of AI that provides systems the ability to automatically learn and improve from experience without explicit programming. Model In machine learning, a model is the output of a machine learning algorithm trained on data. Model File A file that contains a neural network. In the AI Racing League we store our models in H5 formats Model Collapse When new generative models train on AI-generated content and gradually degenerate as a result. Tech Target Natural Language Processing (NLP) A field of AI that focuses on the interaction between computers and human language. Overfitting A modeling error in machine learning where a function fits the training data too closely, reducing its performance on new, unseen data. Reinforcement Learning A type of machine learning where an agent learns to make decisions by interacting with an environment to achieve a goal. Supervised Learning A machine learning paradigm where the algorithm learns from labeled training data, and makes predictions based on that data. Unsupervised Learning A machine learning paradigm where the algorithm learns from unlabeled data, without a specific task like classification or prediction. Perceptron A type of artificial neuron or node used in a neural network, which takes a set of inputs and produces an output by applying a set of weights and an activation function. Training Set A subset of a data set used to train a machine learning model. Validation Set A subset of a data set used to assess the performance of a machine learning model during the training phase, but is not used for training itself. Genetic Algorithm A search heuristic inspired by the process of natural selection, used to find approximate solutions to optimization and search problems. Hyperparameter A parameter whose value is set before the training process begins, influencing the performance of the machine learning model. Inference The process of using a trained machine learning model to make predictions or decisions. K-Means A clustering algorithm that partitions a data set into 'K' number of clusters. Loss Function A function that calculates the difference between the predicted output and the actual label in supervised learning. Overfitting A modeling error where a machine learning algorithm captures noise in the training data, resulting in poor performance on new, unseen data. Precision and Recall Metrics used to evaluate the quality of models in classification tasks, especially where imbalances or specific costs are associated with errors. Query A request for data or information from a database, often in the context of data retrieval or modification. Random Forest An ensemble learning method that constructs multiple decision trees and outputs the class that is the mode of the classes of the individual trees. Stochastic Gradient Descent (SGD) An optimization algorithm often used for finding the minimum of a function, commonly used in machine learning algorithms. Test Set A subset of the data set used to assess the performance of a fully-trained machine learning model. Underfitting A modeling error where a machine learning algorithm cannot capture the underlying trend in the data, resulting in poor performance. Vector An ordered list of numbers, often used to represent a data point in machine learning.","title":"Glossary"},{"location":"glossary/#glossary-of-ai-terms","text":"","title":"Glossary of AI Terms"},{"location":"glossary/#activation-function","text":"A mathematical function applied to a node in a neural network, dictating the output given an input or set of inputs.","title":"Activation Function"},{"location":"glossary/#algorithm","text":"A set of step-by-step procedures or formulas for solving a problem or performing a task.","title":"Algorithm"},{"location":"glossary/#artificial-intelligence-ai","text":"The simulation of human intelligence processes by machines, especially computer systems.","title":"Artificial Intelligence (AI)"},{"location":"glossary/#artificial-neural-network-ann","text":"A computing model inspired by biological neural networks, designed to recognize patterns.","title":"Artificial Neural Network (ANN)"},{"location":"glossary/#backpropagation","text":"An algorithm for adjusting the weights in a neural network based on the error in the output.","title":"Backpropagation"},{"location":"glossary/#batch-size","text":"The number of training examples utilized in one iteration during the training of a neural network.","title":"Batch Size"},{"location":"glossary/#bias","text":"In machine learning, a term that describes the skewness in data or algorithmic decisions. In neural networks, a constant added to a weighted sum before the activation function.","title":"Bias"},{"location":"glossary/#classification","text":"A type of supervised learning where an algorithm categorizes input data into predefined classes.","title":"Classification"},{"location":"glossary/#clustering","text":"A type of unsupervised learning that groups similar data points together based on their features.","title":"Clustering"},{"location":"glossary/#convolutional-neural-network-cnn","text":"A type of neural network commonly used in image processing that uses convolutional layers to filter inputs for useful information.","title":"Convolutional Neural Network (CNN)"},{"location":"glossary/#decision-tree","text":"A flowchart-like tree structure used in machine learning for making decisions based on multiple conditions.","title":"Decision Tree"},{"location":"glossary/#data-mining","text":"The process of discovering patterns and knowledge from large amounts of data.","title":"Data Mining"},{"location":"glossary/#data-set","text":"A collection of related data points used for the purposes of analysis or training machine learning models.","title":"Data Set"},{"location":"glossary/#deep-learning","text":"A subset of machine learning involving algorithms inspired by the structure and function of the brain called artificial neural networks.","title":"Deep Learning"},{"location":"glossary/#epoch","text":"One complete forward and backward pass of all the training examples in a data set.","title":"Epoch"},{"location":"glossary/#feature","text":"An individual measurable property of the data being observed, often used as input variables for machine learning models.","title":"Feature"},{"location":"glossary/#heuristic","text":"A problem-solving technique that employs a practical method to find a sufficient, though not always optimal, solution.","title":"Heuristic"},{"location":"glossary/#machine-learning-ml","text":"A subset of AI that provides systems the ability to automatically learn and improve from experience without explicit programming.","title":"Machine Learning (ML)"},{"location":"glossary/#model","text":"In machine learning, a model is the output of a machine learning algorithm trained on data.","title":"Model"},{"location":"glossary/#model-file","text":"A file that contains a neural network. In the AI Racing League we store our models in H5 formats","title":"Model File"},{"location":"glossary/#model-collapse","text":"When new generative models train on AI-generated content and gradually degenerate as a result. Tech Target","title":"Model Collapse"},{"location":"glossary/#natural-language-processing-nlp","text":"A field of AI that focuses on the interaction between computers and human language.","title":"Natural Language Processing (NLP)"},{"location":"glossary/#overfitting","text":"A modeling error in machine learning where a function fits the training data too closely, reducing its performance on new, unseen data.","title":"Overfitting"},{"location":"glossary/#reinforcement-learning","text":"A type of machine learning where an agent learns to make decisions by interacting with an environment to achieve a goal.","title":"Reinforcement Learning"},{"location":"glossary/#supervised-learning","text":"A machine learning paradigm where the algorithm learns from labeled training data, and makes predictions based on that data.","title":"Supervised Learning"},{"location":"glossary/#unsupervised-learning","text":"A machine learning paradigm where the algorithm learns from unlabeled data, without a specific task like classification or prediction.","title":"Unsupervised Learning"},{"location":"glossary/#perceptron","text":"A type of artificial neuron or node used in a neural network, which takes a set of inputs and produces an output by applying a set of weights and an activation function.","title":"Perceptron"},{"location":"glossary/#training-set","text":"A subset of a data set used to train a machine learning model.","title":"Training Set"},{"location":"glossary/#validation-set","text":"A subset of a data set used to assess the performance of a machine learning model during the training phase, but is not used for training itself.","title":"Validation Set"},{"location":"glossary/#genetic-algorithm","text":"A search heuristic inspired by the process of natural selection, used to find approximate solutions to optimization and search problems.","title":"Genetic Algorithm"},{"location":"glossary/#hyperparameter","text":"A parameter whose value is set before the training process begins, influencing the performance of the machine learning model.","title":"Hyperparameter"},{"location":"glossary/#inference","text":"The process of using a trained machine learning model to make predictions or decisions.","title":"Inference"},{"location":"glossary/#k-means","text":"A clustering algorithm that partitions a data set into 'K' number of clusters.","title":"K-Means"},{"location":"glossary/#loss-function","text":"A function that calculates the difference between the predicted output and the actual label in supervised learning.","title":"Loss Function"},{"location":"glossary/#overfitting_1","text":"A modeling error where a machine learning algorithm captures noise in the training data, resulting in poor performance on new, unseen data.","title":"Overfitting"},{"location":"glossary/#precision-and-recall","text":"Metrics used to evaluate the quality of models in classification tasks, especially where imbalances or specific costs are associated with errors.","title":"Precision and Recall"},{"location":"glossary/#query","text":"A request for data or information from a database, often in the context of data retrieval or modification.","title":"Query"},{"location":"glossary/#random-forest","text":"An ensemble learning method that constructs multiple decision trees and outputs the class that is the mode of the classes of the individual trees.","title":"Random Forest"},{"location":"glossary/#stochastic-gradient-descent-sgd","text":"An optimization algorithm often used for finding the minimum of a function, commonly used in machine learning algorithms.","title":"Stochastic Gradient Descent (SGD)"},{"location":"glossary/#test-set","text":"A subset of the data set used to assess the performance of a fully-trained machine learning model.","title":"Test Set"},{"location":"glossary/#underfitting","text":"A modeling error where a machine learning algorithm cannot capture the underlying trend in the data, resulting in poor performance.","title":"Underfitting"},{"location":"glossary/#vector","text":"An ordered list of numbers, often used to represent a data point in machine learning.","title":"Vector"},{"location":"quotes/","text":"AI Quotes Cheaper by the Month AI is becoming more powerful and radically cheaper by the month\u2014what was computationally impossible, or would cost tens of millions of dollars a few years ago, is now widespread. Mustafa Suleyman September 1st, 2023, Time","title":"Quotes"},{"location":"quotes/#ai-quotes","text":"","title":"AI Quotes"},{"location":"quotes/#cheaper-by-the-month","text":"AI is becoming more powerful and radically cheaper by the month\u2014what was computationally impossible, or would cost tens of millions of dollars a few years ago, is now widespread. Mustafa Suleyman September 1st, 2023, Time","title":"Cheaper by the Month"},{"location":"resources/","text":"Resources for teaching AI to Students Ages 8-18 We aspire to have a collection of AI activities for students that are both safe and fun. There are many AI demos available but many of them are trained on a wide range of data that could include adult content. So we need to be cautious before we recommend a specific site. With that said, here are some demonstrations that can be both safe and fun. We hope you will try them out and let us know what activities are useful in your mentoring and classrooms. Quickdraw (with Google) This fun game challenges kids to draw a picture of a named item. The students have 20 seconds to draw the item. As they draw the computer says \"I see X\" or \"I see Y\". If the computer guesses correctly you go on to the next item. You get six items in a session. Sample items are cup, clock, cellphone, lighthouse, bear etc. QuickDraw website Teachable Machine This Google site is a great way to teach simple machine learning to students. Although it works on images and sounds, we use it mostly in the image mode. The site assumes you have a web camera and can hold items up to the camera for gathering training images. Students can hold up apple, a cell phone and a book and see if the computer can learn the images and then classify a new image. Teachable Machine Google TensorFlow Playground This site is excellent for helping you visualize how neural networks classify object. The interactive tool allows you to change the number of neurons and layers in a neural network and see how these changes impact a simple classifier. Google TensorFlow Playground AI and Ethics for Middle School This is a curriculum designed by MIT professor Blakeley H. Payne. The curriculum focuses on topics such as training, classification and bias. AI for Middle School (Course Description() AI for Middle School (Google Docs) Chatbots for Kids Chatbots can be fun, but there are many concerns about chatbots with feedback that generate bias and adult content. Althogh we m adult supervision for all chatbots, here is one that is relativly benign. https://www.pandorabots.com/mitsuku/ If you are interested in writing your own chatbot for a classroom activity, here are some guildlines: Chatbot Design for Kids Story Generation We are continually looking for a kid safe version of an AI story generator. If you have students that are 18 or older than you might try the Talk to Transformer demonstration. Since a small percent of their traning set is fan fiction that does have adult content, please be careful. Talk to Transformer (the free version currently shut down) Teaching AI Book This book is written from the perspective of a teacher without a strong background in AI. It includes many useful references. Teaching AI: Exploring New Frontiers for Learning by Michelle Zimmerman Conversational AI Both chatbots and Alexa apps (called \"skills\") are examples of conversational AI. Designing AI for the classroom is currently difficult. Amazon Alexa Confersational AI <<<<<<< HEAD Deep Learning Videos ======= Grammerly Tool - AI for checking your writing The Grammerly tool can be used to not only check your grammer but it will also suggest alternate words and see if the tone of your document matches your goals. Grammerly Note that there is a plugin for VSCode here: Grammerly VSCode Plugin TabNine - AI for compleating your code The TabNine extension for your Integrated Development Environment (IDE) is a good example of using a predictive model to suggest what text should be next in a sentence. TabNine Videos eddbb52a8962f7c0e86d3d015f08826db266b21e The 3Blue 1 Brown Series by Grant Sanderson This series has some of the best video production value of all the intoductary videos on deep learning. It is part of the high quality Three Blue One Brown series set of videos done by Grant Sanderson of Stanford University and the Kahn Academy. The videos include many animated visual explainations that use both motion and color to describe mathematics. What is a neural network? The example used is the popular MNIST hand written digit recognition example. What we call the \"Hello world\" of AI. It walks though the process of representing digits in a 28X28 matrix of values for each digit and passing the images through a deep learing neural network. This video covers the concepts of image recognition digit recognition image pixels pixel brightness grayscale values neural networks input layers hidden layers output layers activation activation functions probability decomposition training weighted sums normalization the sigmoid function bias parameter counting (sum of weights and bias) vector representations matrix representations linear algebra transformation functions. At the end they also add a footnote on how ReLU is more common than Sibmoid. [What is a neural network?] https://www.youtube.com/watch?time_continue=5&v=aircAruvnKk&feature=emb_logo <<<<<<< HEAD Gradient descent, how neural networks learn This is the second video. It includes concepts such as review of prior video in handwritten digit recognition and the structure of a neural network model the training process labels in the training set generalization test data (what you network has never seen before) testing the precision of test data MNIST training database (LeCun, Cortes, and Burges) MNIST database Finding minima like in Calculus where the slop is zero Slop of zero is a miniumum of an error Random initialization of weights A cost function - a way of telling a neural network how it can improve Adding the squares of the differences Averaging the cost of the error functions for all input data Finding the lowest cost and minimumization using a 2D visualization Using the slope as a hint on how to adjust the weights The setting slope (derative) to zero to find a minium cost Balls rolling down a hill metaphore and local miniums The role of step size when moving toward a minimum cost Moving to a 2D input model and a 3D error function Gradient function and direction of steepest decent Finding the right downhill direction vector Reference to the backpropagation algorithm (next video) Association of learning to cost function minimumization Why smoothness of a cost function is critical for gradient decent to work and how using decimal values for activation node values is important How our hope of looking for components of a line in each layer didn't work at all The neural network does not \"understand\" the shape - it can't draw any shape. There is no interpretpetable knowledge in the individual weights. This is old technology - based on a multi-layer perceptron - we can do better with modern algorithms This primitive approach is \"dumb\" - but required for us to understand modern variations like CNN and LSTM References from videos: [Neural Networks and Deep Learning by Michael Nielsen] (http://neuralnetworksanddeeplearning.com/) this is a free online book. Michael's strategy is to promote a deep understanding of the core concepts and their relationships rather than memorizing a shallow defintions of hundreds of terms. CS231n video ======= eddbb52a8962f7c0e86d3d015f08826db266b21e","title":"Resources"},{"location":"resources/#resources-for-teaching-ai-to-students-ages-8-18","text":"We aspire to have a collection of AI activities for students that are both safe and fun. There are many AI demos available but many of them are trained on a wide range of data that could include adult content. So we need to be cautious before we recommend a specific site. With that said, here are some demonstrations that can be both safe and fun. We hope you will try them out and let us know what activities are useful in your mentoring and classrooms.","title":"Resources for teaching AI to Students Ages 8-18"},{"location":"resources/#quickdraw-with-google","text":"This fun game challenges kids to draw a picture of a named item. The students have 20 seconds to draw the item. As they draw the computer says \"I see X\" or \"I see Y\". If the computer guesses correctly you go on to the next item. You get six items in a session. Sample items are cup, clock, cellphone, lighthouse, bear etc. QuickDraw website","title":"Quickdraw (with Google)"},{"location":"resources/#teachable-machine","text":"This Google site is a great way to teach simple machine learning to students. Although it works on images and sounds, we use it mostly in the image mode. The site assumes you have a web camera and can hold items up to the camera for gathering training images. Students can hold up apple, a cell phone and a book and see if the computer can learn the images and then classify a new image. Teachable Machine","title":"Teachable Machine"},{"location":"resources/#google-tensorflow-playground","text":"This site is excellent for helping you visualize how neural networks classify object. The interactive tool allows you to change the number of neurons and layers in a neural network and see how these changes impact a simple classifier. Google TensorFlow Playground","title":"Google TensorFlow Playground"},{"location":"resources/#ai-and-ethics-for-middle-school","text":"This is a curriculum designed by MIT professor Blakeley H. Payne. The curriculum focuses on topics such as training, classification and bias. AI for Middle School (Course Description() AI for Middle School (Google Docs)","title":"AI and Ethics for Middle School"},{"location":"resources/#chatbots-for-kids","text":"Chatbots can be fun, but there are many concerns about chatbots with feedback that generate bias and adult content. Althogh we m adult supervision for all chatbots, here is one that is relativly benign. https://www.pandorabots.com/mitsuku/ If you are interested in writing your own chatbot for a classroom activity, here are some guildlines: Chatbot Design for Kids","title":"Chatbots for Kids"},{"location":"resources/#story-generation","text":"We are continually looking for a kid safe version of an AI story generator. If you have students that are 18 or older than you might try the Talk to Transformer demonstration. Since a small percent of their traning set is fan fiction that does have adult content, please be careful. Talk to Transformer (the free version currently shut down)","title":"Story Generation"},{"location":"resources/#teaching-ai-book","text":"This book is written from the perspective of a teacher without a strong background in AI. It includes many useful references. Teaching AI: Exploring New Frontiers for Learning by Michelle Zimmerman","title":"Teaching AI Book"},{"location":"resources/#conversational-ai","text":"Both chatbots and Alexa apps (called \"skills\") are examples of conversational AI. Designing AI for the classroom is currently difficult. Amazon Alexa Confersational AI <<<<<<< HEAD","title":"Conversational AI"},{"location":"resources/#deep-learning-videos","text":"=======","title":"Deep Learning Videos"},{"location":"resources/#grammerly-tool-ai-for-checking-your-writing","text":"The Grammerly tool can be used to not only check your grammer but it will also suggest alternate words and see if the tone of your document matches your goals. Grammerly Note that there is a plugin for VSCode here: Grammerly VSCode Plugin","title":"Grammerly Tool - AI for checking your writing"},{"location":"resources/#tabnine-ai-for-compleating-your-code","text":"The TabNine extension for your Integrated Development Environment (IDE) is a good example of using a predictive model to suggest what text should be next in a sentence. TabNine","title":"TabNine - AI for compleating your code"},{"location":"resources/#videos","text":"eddbb52a8962f7c0e86d3d015f08826db266b21e","title":"Videos"},{"location":"resources/#the-3blue-1-brown-series-by-grant-sanderson","text":"This series has some of the best video production value of all the intoductary videos on deep learning. It is part of the high quality Three Blue One Brown series set of videos done by Grant Sanderson of Stanford University and the Kahn Academy. The videos include many animated visual explainations that use both motion and color to describe mathematics.","title":"The 3Blue 1 Brown Series by Grant Sanderson"},{"location":"resources/#what-is-a-neural-network","text":"The example used is the popular MNIST hand written digit recognition example. What we call the \"Hello world\" of AI. It walks though the process of representing digits in a 28X28 matrix of values for each digit and passing the images through a deep learing neural network. This video covers the concepts of image recognition digit recognition image pixels pixel brightness grayscale values neural networks input layers hidden layers output layers activation activation functions probability decomposition training weighted sums normalization the sigmoid function bias parameter counting (sum of weights and bias) vector representations matrix representations linear algebra transformation functions. At the end they also add a footnote on how ReLU is more common than Sibmoid. [What is a neural network?] https://www.youtube.com/watch?time_continue=5&v=aircAruvnKk&feature=emb_logo <<<<<<< HEAD","title":"What is a neural network?"},{"location":"resources/#gradient-descent-how-neural-networks-learn","text":"This is the second video. It includes concepts such as review of prior video in handwritten digit recognition and the structure of a neural network model the training process labels in the training set generalization test data (what you network has never seen before) testing the precision of test data MNIST training database (LeCun, Cortes, and Burges) MNIST database Finding minima like in Calculus where the slop is zero Slop of zero is a miniumum of an error Random initialization of weights A cost function - a way of telling a neural network how it can improve Adding the squares of the differences Averaging the cost of the error functions for all input data Finding the lowest cost and minimumization using a 2D visualization Using the slope as a hint on how to adjust the weights The setting slope (derative) to zero to find a minium cost Balls rolling down a hill metaphore and local miniums The role of step size when moving toward a minimum cost Moving to a 2D input model and a 3D error function Gradient function and direction of steepest decent Finding the right downhill direction vector Reference to the backpropagation algorithm (next video) Association of learning to cost function minimumization Why smoothness of a cost function is critical for gradient decent to work and how using decimal values for activation node values is important How our hope of looking for components of a line in each layer didn't work at all The neural network does not \"understand\" the shape - it can't draw any shape. There is no interpretpetable knowledge in the individual weights. This is old technology - based on a multi-layer perceptron - we can do better with modern algorithms This primitive approach is \"dumb\" - but required for us to understand modern variations like CNN and LSTM","title":"Gradient descent, how neural networks learn"},{"location":"resources/#references-from-videos","text":"[Neural Networks and Deep Learning by Michael Nielsen] (http://neuralnetworksanddeeplearning.com/) this is a free online book. Michael's strategy is to promote a deep understanding of the core concepts and their relationships rather than memorizing a shallow defintions of hundreds of terms. CS231n video ======= eddbb52a8962f7c0e86d3d015f08826db266b21e","title":"References from videos:"},{"location":"suggested-projects/","text":"Suggested Projects Embeddings and Recommendations Recognizing Digits Representing Knowledge Graph knowledge representation using Python NetworkX libraries What is a graph Graph algorithms Graph machine learning","title":"Suggested Projects"},{"location":"suggested-projects/#suggested-projects","text":"","title":"Suggested Projects"},{"location":"suggested-projects/#embeddings-and-recommendations","text":"","title":"Embeddings and Recommendations"},{"location":"suggested-projects/#recognizing-digits","text":"","title":"Recognizing Digits"},{"location":"suggested-projects/#representing-knowledge","text":"Graph knowledge representation using Python NetworkX libraries What is a graph Graph algorithms Graph machine learning","title":"Representing Knowledge"},{"location":"labs/","text":"AI Hands-On Labs Quick, Draw! Quick Draw Lab Teachable Machine Teachable Machine Bias in BERT Bias in BERT Lab HuggingFace HuggingFace Generative AI Generative AI Lab Llama Llama Lab Tensorflow Playground Tensorflow Playground","title":"Introduction"},{"location":"labs/#ai-hands-on-labs","text":"","title":"AI Hands-On Labs"},{"location":"labs/#quick-draw","text":"Quick Draw Lab","title":"Quick, Draw!"},{"location":"labs/#teachable-machine","text":"Teachable Machine","title":"Teachable Machine"},{"location":"labs/#bias-in-bert","text":"Bias in BERT Lab","title":"Bias in BERT"},{"location":"labs/#huggingface","text":"HuggingFace","title":"HuggingFace"},{"location":"labs/#generative-ai","text":"Generative AI Lab","title":"Generative AI"},{"location":"labs/#llama","text":"Llama Lab","title":"Llama"},{"location":"labs/#tensorflow-playground","text":"Tensorflow Playground","title":"Tensorflow Playground"},{"location":"labs/bias/","text":"Bias in Large-Language Models In this lab, we will explore the topic of gender bias in large-language models. Specifically, we will look at how occupations are biased in the BERT large-language model. !!! Warning This lab can be controversial. It has shown to create strong emotions about fairness in our society. Dan's Blog","title":"Bias in Large-Language Models"},{"location":"labs/bias/#bias-in-large-language-models","text":"In this lab, we will explore the topic of gender bias in large-language models. Specifically, we will look at how occupations are biased in the BERT large-language model. !!! Warning This lab can be controversial. It has shown to create strong emotions about fairness in our society. Dan's Blog","title":"Bias in Large-Language Models"},{"location":"labs/generative-ai/","text":"Generative AI We explored the topic of \"Wan computers generate X\" through the lens of products that were pioneers in this area. We focus on image, text, code generation and data analysis. Image Generation OpenAI Dall-E 2 MidJourney Stable Diffusion Text Generation ChatGPT Google BARD Anthropic AI Code Generation GitHub Copilot - integrated into your IDE. GPT-4 - the most powerful model to date. Amazon CodeWhisperer - Free Amazon tool. Data Analysis We can upload a file of data to GhatGPT and it will do analysis of the data. OpenAI ChatGPT Enterprise","title":"Generative AI"},{"location":"labs/generative-ai/#generative-ai","text":"We explored the topic of \"Wan computers generate X\" through the lens of products that were pioneers in this area. We focus on image, text, code generation and data analysis.","title":"Generative AI"},{"location":"labs/generative-ai/#image-generation","text":"OpenAI Dall-E 2 MidJourney Stable Diffusion","title":"Image Generation"},{"location":"labs/generative-ai/#text-generation","text":"ChatGPT Google BARD Anthropic AI","title":"Text Generation"},{"location":"labs/generative-ai/#code-generation","text":"GitHub Copilot - integrated into your IDE. GPT-4 - the most powerful model to date. Amazon CodeWhisperer - Free Amazon tool.","title":"Code Generation"},{"location":"labs/generative-ai/#data-analysis","text":"We can upload a file of data to GhatGPT and it will do analysis of the data. OpenAI ChatGPT Enterprise","title":"Data Analysis"},{"location":"labs/huggingface/","text":"HuggingFace HuggingFace is a \"social media\" site for AI developers. It is similar to Facebook, but instead of connecting people to people, it connects models to many other things such as: Datasets Web Applications (Spaces) Projects Organizations People Architectures Models HuggingFace Models Note the number of models and the popularity of models. Note the \"faceted search\" to narrow models by type. Spaces Spaces are web applications that use an machine learning model. HuggingFace Spaces They are written using Python Streamlit and Gradio libraries. You can create your own applications by just doing a copy/paste of other code.","title":"Hugging Face"},{"location":"labs/huggingface/#huggingface","text":"HuggingFace is a \"social media\" site for AI developers. It is similar to Facebook, but instead of connecting people to people, it connects models to many other things such as: Datasets Web Applications (Spaces) Projects Organizations People Architectures","title":"HuggingFace"},{"location":"labs/huggingface/#models","text":"HuggingFace Models Note the number of models and the popularity of models. Note the \"faceted search\" to narrow models by type.","title":"Models"},{"location":"labs/huggingface/#spaces","text":"Spaces are web applications that use an machine learning model. HuggingFace Spaces They are written using Python Streamlit and Gradio libraries. You can create your own applications by just doing a copy/paste of other code.","title":"Spaces"},{"location":"labs/llama/","text":"Llama https://huggingface.co/spaces/codellama/codellama-13b-chat","title":"Llama"},{"location":"labs/llama/#llama","text":"https://huggingface.co/spaces/codellama/codellama-13b-chat","title":"Llama"},{"location":"labs/quickdraw/","text":"Quick Draw Lab Quick Draw is a fun game that can be played by anyone who can use a mouse. It is a machine-learning web application that will try to guess what you are drawing. It is a great way to introduce kids to machine learning and prediction. QuickDraw is similar to the game Pictionary in that the player only has a limited time to draw and item. But in this case, the computer is trying to guess what you are drawing. QuickDraw was created by the engineers at Google and is kid-safe. It is free and does not gather and store any personal information. How it Works QuickDraw will suggest an item to draw such as \"bicycle\". You then have 20 seconds to draw that item while an AI agent will try to guess what you are drawing. This repeats six times. At the end of the game Quick Steps Have kids play one or more rounds of QuickDraw. Then ask some questions: How long did it take QuickDraw to guess what you were trying to draw? How good was it at guessing? Were all the drawing the same level of difficulty to draw? How many items do you think it was comparing your drawing with? How do you think QuickDraw \"learned\" to match drawings and the names of items? How is \"prediction\" involved in QuickDraw? Could QuickDraw give you detailed explanations of why it guessed specific items? When is it important for an AI to give you explanations of why it has made a decision? Is explainable AI important if healthcare? What is \"similarity\" in the context of QuickDraw? Look at the data used to train QuickDraw. What did you learn from this data? Look at the categories of items. Is this list of items biased in any way? Are insects fairly represented? If you had to write your own version of QuickDraw, how would you go about doing it? More to Explore If your class has some experience with Python, have them do a search for \"Quick, Draw\" on GitHub. What do you find? References Anubhav Shrimal's GitHub repo Chandra Kanth GitHub repo Machine Learning on Wikipedia Explainable AI on Wikipedia","title":"Quick, Draw"},{"location":"labs/quickdraw/#quick-draw-lab","text":"Quick Draw is a fun game that can be played by anyone who can use a mouse. It is a machine-learning web application that will try to guess what you are drawing. It is a great way to introduce kids to machine learning and prediction. QuickDraw is similar to the game Pictionary in that the player only has a limited time to draw and item. But in this case, the computer is trying to guess what you are drawing. QuickDraw was created by the engineers at Google and is kid-safe. It is free and does not gather and store any personal information.","title":"Quick Draw Lab"},{"location":"labs/quickdraw/#how-it-works","text":"QuickDraw will suggest an item to draw such as \"bicycle\". You then have 20 seconds to draw that item while an AI agent will try to guess what you are drawing. This repeats six times. At the end of the game Quick","title":"How it Works"},{"location":"labs/quickdraw/#steps","text":"Have kids play one or more rounds of QuickDraw. Then ask some questions: How long did it take QuickDraw to guess what you were trying to draw? How good was it at guessing? Were all the drawing the same level of difficulty to draw? How many items do you think it was comparing your drawing with? How do you think QuickDraw \"learned\" to match drawings and the names of items? How is \"prediction\" involved in QuickDraw? Could QuickDraw give you detailed explanations of why it guessed specific items? When is it important for an AI to give you explanations of why it has made a decision? Is explainable AI important if healthcare? What is \"similarity\" in the context of QuickDraw? Look at the data used to train QuickDraw. What did you learn from this data? Look at the categories of items. Is this list of items biased in any way? Are insects fairly represented? If you had to write your own version of QuickDraw, how would you go about doing it?","title":"Steps"},{"location":"labs/quickdraw/#more-to-explore","text":"If your class has some experience with Python, have them do a search for \"Quick, Draw\" on GitHub. What do you find?","title":"More to Explore"},{"location":"labs/quickdraw/#references","text":"Anubhav Shrimal's GitHub repo Chandra Kanth GitHub repo Machine Learning on Wikipedia Explainable AI on Wikipedia","title":"References"},{"location":"labs/teachable-machine/","text":"Teachable Machine Teachable Machine With Google","title":"Teachable Machine"},{"location":"labs/teachable-machine/#teachable-machine","text":"Teachable Machine With Google","title":"Teachable Machine"},{"location":"labs/trends/","text":"Trends In this lab, we will ask students to think about some of the trends that will have the most impact on their careers and their lives. Key Questions for this lab: What are the key trends in AI? What is Moore's Law? What is linear growth vs. exponential growth? What types of jobs will AI influence? What is a knowledge worker? Background: Moore's Law Open LLM Leaderboard Look at the following site: Open LLM Leaderboard Note that if you are given a multiple-choice test with four possible answers, and you randomly guessed one answer, you would have a 25% chance of being correct. That is why the \"baseline\" of the leaderboard is 25. Sample AI Question with a Four-Choice Answer \"Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people? Answer:\" The air stays cleaner. Cars can travel at faster speeds. The skills of the drivers improve. It becomes safer to drive on the roads. Potting Progress Contextual AI Plotting Progress 4 Charts That Show Why AI Progress Is Unlikely to Slow Down","title":"Trends"},{"location":"labs/trends/#trends","text":"In this lab, we will ask students to think about some of the trends that will have the most impact on their careers and their lives. Key Questions for this lab: What are the key trends in AI? What is Moore's Law? What is linear growth vs. exponential growth? What types of jobs will AI influence? What is a knowledge worker?","title":"Trends"},{"location":"labs/trends/#background-moores-law","text":"","title":"Background: Moore's Law"},{"location":"labs/trends/#open-llm-leaderboard","text":"Look at the following site: Open LLM Leaderboard Note that if you are given a multiple-choice test with four possible answers, and you randomly guessed one answer, you would have a 25% chance of being correct. That is why the \"baseline\" of the leaderboard is 25.","title":"Open LLM Leaderboard"},{"location":"labs/trends/#sample-ai-question-with-a-four-choice-answer","text":"\"Question: Cities control the amount of pollution that is allowed to come from cars. How does this most likely help people? Answer:\" The air stays cleaner. Cars can travel at faster speeds. The skills of the drivers improve. It becomes safer to drive on the roads.","title":"Sample AI Question with a Four-Choice Answer"},{"location":"labs/trends/#potting-progress","text":"Contextual AI Plotting Progress 4 Charts That Show Why AI Progress Is Unlikely to Slow Down","title":"Potting Progress"},{"location":"labs/neural-network/","text":"Generate a Neural Network Diagram Using ChatGPT First Prompt ```linenums=\"0\" Generate a p5.js file that draws a three-layer neural network. The input is on the left. The output is on the right. The layer on the left should be labeled \"Input\". ## Response [ Neural Network Demo Version 1 ]( v1 . html ) ## Second Prompt ``` linenums = \"0\" Add slider controls that change the number of layers and the number of neurons in a layer . Third Prompt ```linenums=\"0\" Add labels to the sliders. Add a total parameter count at the bottom of the display. The parameter count is the total number of weights and biases in the neural network. ## Fourth Prompt ``` linenums = \"0\" What is the formula for the total number of parameters in terms of layers and neurons in a layer ? In a simple fully connected feedforward neural network with uniform layers ( i . e . , each layer has the same number of neurons ), the total number of parameters is given by the sum of the weights and biases in the network . If \ufffd n is the number of neurons in each layer , and \ufffd L is the number of layers , then : There are ( \ufffd \u2212 1 ) ( L \u2212 1 ) sets of weights between layers . Each set of weights between two adjacent layers would have \ufffd \u00d7 \ufffd n \u00d7 n parameters ( weights ) . Total Weights = ( \ufffd \u2212 1 ) \u00d7 \ufffd \u00d7 \ufffd Total Weights = ( L \u2212 1 ) \u00d7 n \u00d7 n Additionally , each layer ( except the input layer ) has \ufffd n biases . Total Biases = ( \ufffd \u2212 1 ) \u00d7 \ufffd Total Biases = ( L \u2212 1 ) \u00d7 n So , the total number of parameters is the sum of the weights and biases : Total Parameters = Total Weights + Total Biases = ( \ufffd \u2212 1 ) \u00d7 \ufffd \u00d7 \ufffd + ( \ufffd \u2212 1 ) \u00d7 \ufffd Total Parameters = Total Weights + Total Biases = ( L \u2212 1 ) \u00d7 n \u00d7 n + ( L \u2212 1 ) \u00d7 n Total Parameters = ( \ufffd \u2212 1 ) \u00d7 \ufffd \u00d7 ( \ufffd + 1 ) Total Parameters = ( L \u2212 1 ) \u00d7 n \u00d7 ( n + 1 ) This formula assumes a simple neural network architecture where each layer has the same number of neurons . If the layers have varying numbers of neurons , the formula would need to be adjusted accordingly .","title":"Generate a Neural Network Diagram Using ChatGPT"},{"location":"labs/neural-network/#generate-a-neural-network-diagram-using-chatgpt","text":"","title":"Generate a Neural Network Diagram Using ChatGPT"},{"location":"labs/neural-network/#first-prompt","text":"```linenums=\"0\" Generate a p5.js file that draws a three-layer neural network. The input is on the left. The output is on the right. The layer on the left should be labeled \"Input\". ## Response [ Neural Network Demo Version 1 ]( v1 . html ) ## Second Prompt ``` linenums = \"0\" Add slider controls that change the number of layers and the number of neurons in a layer .","title":"First Prompt"},{"location":"labs/neural-network/#third-prompt","text":"```linenums=\"0\" Add labels to the sliders. Add a total parameter count at the bottom of the display. The parameter count is the total number of weights and biases in the neural network. ## Fourth Prompt ``` linenums = \"0\" What is the formula for the total number of parameters in terms of layers and neurons in a layer ? In a simple fully connected feedforward neural network with uniform layers ( i . e . , each layer has the same number of neurons ), the total number of parameters is given by the sum of the weights and biases in the network . If \ufffd n is the number of neurons in each layer , and \ufffd L is the number of layers , then : There are ( \ufffd \u2212 1 ) ( L \u2212 1 ) sets of weights between layers . Each set of weights between two adjacent layers would have \ufffd \u00d7 \ufffd n \u00d7 n parameters ( weights ) . Total Weights = ( \ufffd \u2212 1 ) \u00d7 \ufffd \u00d7 \ufffd Total Weights = ( L \u2212 1 ) \u00d7 n \u00d7 n Additionally , each layer ( except the input layer ) has \ufffd n biases . Total Biases = ( \ufffd \u2212 1 ) \u00d7 \ufffd Total Biases = ( L \u2212 1 ) \u00d7 n So , the total number of parameters is the sum of the weights and biases : Total Parameters = Total Weights + Total Biases = ( \ufffd \u2212 1 ) \u00d7 \ufffd \u00d7 \ufffd + ( \ufffd \u2212 1 ) \u00d7 \ufffd Total Parameters = Total Weights + Total Biases = ( L \u2212 1 ) \u00d7 n \u00d7 n + ( L \u2212 1 ) \u00d7 n Total Parameters = ( \ufffd \u2212 1 ) \u00d7 \ufffd \u00d7 ( \ufffd + 1 ) Total Parameters = ( L \u2212 1 ) \u00d7 n \u00d7 ( n + 1 ) This formula assumes a simple neural network architecture where each layer has the same number of neurons . If the layers have varying numbers of neurons , the formula would need to be adjusted accordingly .","title":"Third Prompt"},{"location":"labs/tensorflow-playground/","text":"TensorFlow Playground In this lab, we will use the Google TensorFlow Playground web application to build very small neural networks. Our goal is to get a high-level feeling of what neural networks are and how they transform data in layers. The TensorFlow Playground web application is here: https://playground.tensorflow.org/ Starting Simple The best way to start this class is to start with a very simple dataset and see if we can create a simple neural network to determine if an item is in one of two classifications. Once we understand this process, we will make the datasets more complicated and also increase the complexity of the neural network. Basic Concepts Training Data - data we build the neural network with Test Data - data we evaluate how good our neural network is The Ratio of Train to Test - the relative size of the data sets. The default ratio is 50% Epoch - one complete pass through all the training data A Simplified User Interface Simplified UI With Just the Step UI Single Neuron Click the \"Reset the Network\" button in the upper left corner: Note what test loss and training loss are after you do this: This says you are getting a 50% chance of getting the answers correct. Change the weight of the second link to be 10. You should see the width of the line get much larger: Press the Step button and note the change in the Output: Single Exercise How many steps does it take for the \"Test loss\" to drop below 0.01? Regenerate the initial values several times. How do the initial values impace the speed to lower the loss function? Turn the noise up to 10. What happens when the data is \"dirty\"? Click the \"show test data\" button.","title":"Tensorflow Playground"},{"location":"labs/tensorflow-playground/#tensorflow-playground","text":"In this lab, we will use the Google TensorFlow Playground web application to build very small neural networks. Our goal is to get a high-level feeling of what neural networks are and how they transform data in layers. The TensorFlow Playground web application is here: https://playground.tensorflow.org/","title":"TensorFlow Playground"},{"location":"labs/tensorflow-playground/#starting-simple","text":"The best way to start this class is to start with a very simple dataset and see if we can create a simple neural network to determine if an item is in one of two classifications. Once we understand this process, we will make the datasets more complicated and also increase the complexity of the neural network.","title":"Starting Simple"},{"location":"labs/tensorflow-playground/#basic-concepts","text":"Training Data - data we build the neural network with Test Data - data we evaluate how good our neural network is The Ratio of Train to Test - the relative size of the data sets. The default ratio is 50% Epoch - one complete pass through all the training data","title":"Basic Concepts"},{"location":"labs/tensorflow-playground/#a-simplified-user-interface","text":"Simplified UI With Just the Step UI","title":"A Simplified User Interface"},{"location":"labs/tensorflow-playground/#single-neuron","text":"Click the \"Reset the Network\" button in the upper left corner: Note what test loss and training loss are after you do this: This says you are getting a 50% chance of getting the answers correct. Change the weight of the second link to be 10. You should see the width of the line get much larger: Press the Step button and note the change in the Output:","title":"Single Neuron"},{"location":"labs/tensorflow-playground/#single-exercise","text":"How many steps does it take for the \"Test loss\" to drop below 0.01? Regenerate the initial values several times. How do the initial values impace the speed to lower the loss function? Turn the noise up to 10. What happens when the data is \"dirty\"? Click the \"show test data\" button.","title":"Single Exercise"}]}